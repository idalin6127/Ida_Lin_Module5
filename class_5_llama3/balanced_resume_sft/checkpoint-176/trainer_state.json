{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 8.0,
  "eval_steps": 500,
  "global_step": 176,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 2.997352361679077,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 9.0724,
      "step": 2
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 3.0730130672454834,
      "learning_rate": 0.00011999999999999999,
      "loss": 11.1867,
      "step": 4
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 3.7066266536712646,
      "learning_rate": 0.00017999999999999998,
      "loss": 9.2804,
      "step": 6
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 3.4187636375427246,
      "learning_rate": 0.00023999999999999998,
      "loss": 9.4793,
      "step": 8
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 4.3321099281311035,
      "learning_rate": 0.0003,
      "loss": 9.7331,
      "step": 10
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 3.740365505218506,
      "learning_rate": 0.0002998925632224497,
      "loss": 9.0014,
      "step": 12
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 4.033952713012695,
      "learning_rate": 0.00029957040679194776,
      "loss": 8.7699,
      "step": 14
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 4.24626350402832,
      "learning_rate": 0.0002990339921944777,
      "loss": 8.937,
      "step": 16
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 4.258646488189697,
      "learning_rate": 0.00029828408783878324,
      "loss": 7.5514,
      "step": 18
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 3.22210955619812,
      "learning_rate": 0.00029732176795563037,
      "loss": 6.5113,
      "step": 20
    },
    {
      "epoch": 1.0,
      "grad_norm": 5.281742095947266,
      "learning_rate": 0.000296148411058982,
      "loss": 7.2988,
      "step": 22
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 6.1620283126831055,
      "learning_rate": 0.0002947656979712899,
      "loss": 6.3285,
      "step": 24
    },
    {
      "epoch": 1.1818181818181819,
      "grad_norm": 4.651156425476074,
      "learning_rate": 0.0002931756094157332,
      "loss": 5.8909,
      "step": 26
    },
    {
      "epoch": 1.2727272727272727,
      "grad_norm": 4.119163513183594,
      "learning_rate": 0.0002913804231788509,
      "loss": 5.5382,
      "step": 28
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 4.3284831047058105,
      "learning_rate": 0.0002893827108476348,
      "loss": 5.0987,
      "step": 30
    },
    {
      "epoch": 1.4545454545454546,
      "grad_norm": 3.120762825012207,
      "learning_rate": 0.00028718533412575606,
      "loss": 5.3946,
      "step": 32
    },
    {
      "epoch": 1.5454545454545454,
      "grad_norm": 3.822800636291504,
      "learning_rate": 0.00028479144073420234,
      "loss": 5.4349,
      "step": 34
    },
    {
      "epoch": 1.6363636363636362,
      "grad_norm": 4.686867713928223,
      "learning_rate": 0.0002822044599021973,
      "loss": 4.6447,
      "step": 36
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 3.9492764472961426,
      "learning_rate": 0.00027942809745486343,
      "loss": 4.5858,
      "step": 38
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 2.5764846801757812,
      "learning_rate": 0.00027646633050466265,
      "loss": 4.7086,
      "step": 40
    },
    {
      "epoch": 1.9090909090909092,
      "grad_norm": 2.7960808277130127,
      "learning_rate": 0.0002733234017542215,
      "loss": 5.0655,
      "step": 42
    },
    {
      "epoch": 2.0,
      "grad_norm": 3.4320826530456543,
      "learning_rate": 0.0002700038134187002,
      "loss": 4.6561,
      "step": 44
    },
    {
      "epoch": 2.090909090909091,
      "grad_norm": 2.4968209266662598,
      "learning_rate": 0.00026651232077641276,
      "loss": 4.1489,
      "step": 46
    },
    {
      "epoch": 2.1818181818181817,
      "grad_norm": 4.110382080078125,
      "learning_rate": 0.0002628539253569372,
      "loss": 3.9043,
      "step": 48
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 2.532297372817993,
      "learning_rate": 0.00025903386777647154,
      "loss": 4.1499,
      "step": 50
    },
    {
      "epoch": 2.3636363636363638,
      "grad_norm": 2.365386724472046,
      "learning_rate": 0.0002550576202307026,
      "loss": 3.6975,
      "step": 52
    },
    {
      "epoch": 2.4545454545454546,
      "grad_norm": 3.2271926403045654,
      "learning_rate": 0.0002509308786559378,
      "loss": 4.1079,
      "step": 54
    },
    {
      "epoch": 2.5454545454545454,
      "grad_norm": 4.41552734375,
      "learning_rate": 0.00024665955456973154,
      "loss": 3.8001,
      "step": 56
    },
    {
      "epoch": 2.6363636363636362,
      "grad_norm": 4.179531097412109,
      "learning_rate": 0.00024224976660269302,
      "loss": 3.5299,
      "step": 58
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 3.028378486633301,
      "learning_rate": 0.00023770783173360704,
      "loss": 4.5743,
      "step": 60
    },
    {
      "epoch": 2.8181818181818183,
      "grad_norm": 3.1179287433624268,
      "learning_rate": 0.00023304025624042263,
      "loss": 4.6219,
      "step": 62
    },
    {
      "epoch": 2.909090909090909,
      "grad_norm": 18.247940063476562,
      "learning_rate": 0.00022825372638007267,
      "loss": 4.2465,
      "step": 64
    },
    {
      "epoch": 3.0,
      "grad_norm": 2.4441421031951904,
      "learning_rate": 0.00022335509881047497,
      "loss": 3.8371,
      "step": 66
    },
    {
      "epoch": 3.090909090909091,
      "grad_norm": 2.570293664932251,
      "learning_rate": 0.00021835139076843623,
      "loss": 3.9813,
      "step": 68
    },
    {
      "epoch": 3.1818181818181817,
      "grad_norm": 3.1447019577026367,
      "learning_rate": 0.00021324977001752757,
      "loss": 3.848,
      "step": 70
    },
    {
      "epoch": 3.2727272727272725,
      "grad_norm": 2.549950361251831,
      "learning_rate": 0.0002080575445803326,
      "loss": 3.8644,
      "step": 72
    },
    {
      "epoch": 3.3636363636363638,
      "grad_norm": 2.2996363639831543,
      "learning_rate": 0.00020278215226977493,
      "loss": 3.548,
      "step": 74
    },
    {
      "epoch": 3.4545454545454546,
      "grad_norm": 2.53277587890625,
      "learning_rate": 0.00019743115003452357,
      "loss": 3.3398,
      "step": 76
    },
    {
      "epoch": 3.5454545454545454,
      "grad_norm": 2.8077592849731445,
      "learning_rate": 0.00019201220313373607,
      "loss": 3.3714,
      "step": 78
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 3.319408655166626,
      "learning_rate": 0.00018653307415664877,
      "loss": 3.1065,
      "step": 80
    },
    {
      "epoch": 3.7272727272727275,
      "grad_norm": 3.227287530899048,
      "learning_rate": 0.0001810016119027429,
      "loss": 3.1632,
      "step": 82
    },
    {
      "epoch": 3.8181818181818183,
      "grad_norm": 2.822815179824829,
      "learning_rate": 0.0001754257401384145,
      "loss": 3.4424,
      "step": 84
    },
    {
      "epoch": 3.909090909090909,
      "grad_norm": 2.345728874206543,
      "learning_rate": 0.00016981344624625536,
      "loss": 3.4696,
      "step": 86
    },
    {
      "epoch": 4.0,
      "grad_norm": 3.1576712131500244,
      "learning_rate": 0.00016417276978320468,
      "loss": 3.5414,
      "step": 88
    },
    {
      "epoch": 4.090909090909091,
      "grad_norm": 2.610158920288086,
      "learning_rate": 0.00015851179096396112,
      "loss": 3.3986,
      "step": 90
    },
    {
      "epoch": 4.181818181818182,
      "grad_norm": 2.7448666095733643,
      "learning_rate": 0.00015283861908615284,
      "loss": 2.9694,
      "step": 92
    },
    {
      "epoch": 4.2727272727272725,
      "grad_norm": 2.284245729446411,
      "learning_rate": 0.00014716138091384716,
      "loss": 3.7361,
      "step": 94
    },
    {
      "epoch": 4.363636363636363,
      "grad_norm": 2.845381498336792,
      "learning_rate": 0.00014148820903603888,
      "loss": 3.7081,
      "step": 96
    },
    {
      "epoch": 4.454545454545454,
      "grad_norm": 2.677905797958374,
      "learning_rate": 0.00013582723021679532,
      "loss": 3.1427,
      "step": 98
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 2.7975611686706543,
      "learning_rate": 0.00013018655375374467,
      "loss": 2.6207,
      "step": 100
    },
    {
      "epoch": 4.636363636363637,
      "grad_norm": 2.76446795463562,
      "learning_rate": 0.00012457425986158547,
      "loss": 2.5472,
      "step": 102
    },
    {
      "epoch": 4.7272727272727275,
      "grad_norm": 2.3323285579681396,
      "learning_rate": 0.00011899838809725704,
      "loss": 3.0496,
      "step": 104
    },
    {
      "epoch": 4.818181818181818,
      "grad_norm": 2.8466010093688965,
      "learning_rate": 0.0001134669258433512,
      "loss": 2.9219,
      "step": 106
    },
    {
      "epoch": 4.909090909090909,
      "grad_norm": 2.85487699508667,
      "learning_rate": 0.00010798779686626394,
      "loss": 3.3539,
      "step": 108
    },
    {
      "epoch": 5.0,
      "grad_norm": 2.776681423187256,
      "learning_rate": 0.00010256884996547639,
      "loss": 3.6303,
      "step": 110
    },
    {
      "epoch": 5.090909090909091,
      "grad_norm": 2.575958013534546,
      "learning_rate": 9.721784773022504e-05,
      "loss": 3.2922,
      "step": 112
    },
    {
      "epoch": 5.181818181818182,
      "grad_norm": 3.031785011291504,
      "learning_rate": 9.194245541966741e-05,
      "loss": 3.2848,
      "step": 114
    },
    {
      "epoch": 5.2727272727272725,
      "grad_norm": 3.4318363666534424,
      "learning_rate": 8.675022998247239e-05,
      "loss": 3.1908,
      "step": 116
    },
    {
      "epoch": 5.363636363636363,
      "grad_norm": 2.6725914478302,
      "learning_rate": 8.164860923156377e-05,
      "loss": 3.0552,
      "step": 118
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 2.7122802734375,
      "learning_rate": 7.664490118952502e-05,
      "loss": 3.4297,
      "step": 120
    },
    {
      "epoch": 5.545454545454545,
      "grad_norm": 3.409667491912842,
      "learning_rate": 7.174627361992732e-05,
      "loss": 3.1625,
      "step": 122
    },
    {
      "epoch": 5.636363636363637,
      "grad_norm": 2.748147964477539,
      "learning_rate": 6.695974375957733e-05,
      "loss": 3.1598,
      "step": 124
    },
    {
      "epoch": 5.7272727272727275,
      "grad_norm": 3.545281171798706,
      "learning_rate": 6.229216826639293e-05,
      "loss": 2.58,
      "step": 126
    },
    {
      "epoch": 5.818181818181818,
      "grad_norm": 2.985171318054199,
      "learning_rate": 5.775023339730696e-05,
      "loss": 2.4241,
      "step": 128
    },
    {
      "epoch": 5.909090909090909,
      "grad_norm": 3.225771188735962,
      "learning_rate": 5.3340445430268436e-05,
      "loss": 2.8072,
      "step": 130
    },
    {
      "epoch": 6.0,
      "grad_norm": 3.076963186264038,
      "learning_rate": 4.906912134406216e-05,
      "loss": 2.9378,
      "step": 132
    },
    {
      "epoch": 6.090909090909091,
      "grad_norm": 2.6973390579223633,
      "learning_rate": 4.4942379769297435e-05,
      "loss": 3.1412,
      "step": 134
    },
    {
      "epoch": 6.181818181818182,
      "grad_norm": 3.024271249771118,
      "learning_rate": 4.096613222352843e-05,
      "loss": 3.2919,
      "step": 136
    },
    {
      "epoch": 6.2727272727272725,
      "grad_norm": 3.1943230628967285,
      "learning_rate": 3.714607464306281e-05,
      "loss": 2.6207,
      "step": 138
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 3.4295177459716797,
      "learning_rate": 3.348767922358719e-05,
      "loss": 2.8687,
      "step": 140
    },
    {
      "epoch": 6.454545454545454,
      "grad_norm": 3.8442625999450684,
      "learning_rate": 2.9996186581299824e-05,
      "loss": 2.4484,
      "step": 142
    },
    {
      "epoch": 6.545454545454545,
      "grad_norm": 2.865798234939575,
      "learning_rate": 2.6676598245778498e-05,
      "loss": 3.1265,
      "step": 144
    },
    {
      "epoch": 6.636363636363637,
      "grad_norm": 2.8417601585388184,
      "learning_rate": 2.353366949533736e-05,
      "loss": 3.2826,
      "step": 146
    },
    {
      "epoch": 6.7272727272727275,
      "grad_norm": 2.9262759685516357,
      "learning_rate": 2.0571902545136565e-05,
      "loss": 3.04,
      "step": 148
    },
    {
      "epoch": 6.818181818181818,
      "grad_norm": 2.547369956970215,
      "learning_rate": 1.7795540097802668e-05,
      "loss": 2.9546,
      "step": 150
    },
    {
      "epoch": 6.909090909090909,
      "grad_norm": 3.3360695838928223,
      "learning_rate": 1.5208559265797699e-05,
      "loss": 2.27,
      "step": 152
    },
    {
      "epoch": 7.0,
      "grad_norm": 3.2018446922302246,
      "learning_rate": 1.2814665874243907e-05,
      "loss": 3.6413,
      "step": 154
    },
    {
      "epoch": 7.090909090909091,
      "grad_norm": 3.370999574661255,
      "learning_rate": 1.0617289152365227e-05,
      "loss": 2.6855,
      "step": 156
    },
    {
      "epoch": 7.181818181818182,
      "grad_norm": 3.5481040477752686,
      "learning_rate": 8.619576821149105e-06,
      "loss": 3.3744,
      "step": 158
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 2.6326904296875,
      "learning_rate": 6.824390584266737e-06,
      "loss": 3.2321,
      "step": 160
    },
    {
      "epoch": 7.363636363636363,
      "grad_norm": 3.216029167175293,
      "learning_rate": 5.234302028710008e-06,
      "loss": 2.8665,
      "step": 162
    },
    {
      "epoch": 7.454545454545454,
      "grad_norm": 2.963597297668457,
      "learning_rate": 3.851588941018002e-06,
      "loss": 2.8324,
      "step": 164
    },
    {
      "epoch": 7.545454545454545,
      "grad_norm": 3.158266544342041,
      "learning_rate": 2.6782320443696103e-06,
      "loss": 2.8575,
      "step": 166
    },
    {
      "epoch": 7.636363636363637,
      "grad_norm": 2.7694129943847656,
      "learning_rate": 1.7159121612167203e-06,
      "loss": 3.3124,
      "step": 168
    },
    {
      "epoch": 7.7272727272727275,
      "grad_norm": 3.0240836143493652,
      "learning_rate": 9.66007805522262e-07,
      "loss": 2.89,
      "step": 170
    },
    {
      "epoch": 7.818181818181818,
      "grad_norm": 3.306521415710449,
      "learning_rate": 4.2959320805219244e-07,
      "loss": 2.8233,
      "step": 172
    },
    {
      "epoch": 7.909090909090909,
      "grad_norm": 2.878302574157715,
      "learning_rate": 1.0743677755027936e-07,
      "loss": 2.007,
      "step": 174
    },
    {
      "epoch": 8.0,
      "grad_norm": 2.7334794998168945,
      "learning_rate": 0.0,
      "loss": 3.092,
      "step": 176
    }
  ],
  "logging_steps": 2,
  "max_steps": 176,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 8,
  "save_steps": 20,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 71612032352256.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
